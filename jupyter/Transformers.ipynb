{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "966c1c3e-57ec-4a8a-b887-345c84a4e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef4cec3d-183f-4fee-8f2d-a9d3aa329c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 1번 슬롯만 사용하도록 설정\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d63486b8-7b0e-486a-b09e-226ef86f833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 디바이스 확인\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 메모리 증가 방식으로 설정\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "551ebe49-3ef5-430f-b131-ded27ebd3356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "    d_k = tf.cast(tf.shape(key)[-1], tf.float32) #Float32로 형 변환\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dee2dac-37e4-4043-a01b-8782bfa352cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        x = tf.reshape(x, (tf.shape(x)[0], -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        q = self.split_heads(self.wq(x))\n",
    "        k = self.split_heads(self.wk(x))\n",
    "        v = self.split_heads(self.wv(x))\n",
    "        attention_output, _ = scaled_dot_product_attention(q, k, v, mask)\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        attention_output = tf.reshape(attention_output, (tf.shape(attention_output)[0], -1, self.d_model))\n",
    "        return self.dense(attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a12e779b-45a4-4353-ba1e-a89b3a3eaf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position-wise Feed-Forward Network\n",
    "class PositionwiseFeedforward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedforward, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.dense2(self.dense1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b82de80-14ee-4de3-880e-fbf68c0f4732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.positional_encoding = self.positional_encoding(max_len, d_model)\n",
    "\n",
    "    def positional_encoding(self, max_len, d_model):\n",
    "        position = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        pos_enc = np.zeros((max_len, d_model))\n",
    "        pos_enc[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = np.cos(position * div_term)\n",
    "        return tf.constant(pos_enc, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        return x + self.positional_encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e64692f-12f4-4715-ae8f-9704917cda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionwiseFeedforward(d_model, d_ff)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        attn_output = self.dropout(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b965aa3-fb09-4afb-b2e6-c20e7b1eebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, max_len, num_classes, dropout_rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Dense(d_model)\n",
    "        self.positional_encoding = PositionalEncoding(max_len, d_model)\n",
    "        self.transformer_blocks = [TransformerBlock(d_model, num_heads, d_ff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(num_classes, activation = 'sigmoid') #이진분류\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask, training=training)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.final_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1650764-8a77-4bb4-a1a0-c039404be303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩 및 전처리\n",
    "def load_npy_files(folder_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.npy'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            data.append(np.load(file_path))\n",
    "            labels.append(file_name.split('_')[0])  # 파일명에서 라벨 추출 (예시)\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2afcf27b-7325-46a6-8e9b-b3ce9ae2f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save_data(data, labels, save_path, split_ratio=(0.8, 0.1, 0.1)):\n",
    "    # Split data into train, validation, and test sets\n",
    "    train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "        data, labels, test_size=1 - split_ratio[0], stratify=labels\n",
    "    )\n",
    "    valid_data, test_data, valid_labels, test_labels = train_test_split(\n",
    "        temp_data, temp_labels, test_size=split_ratio[2] / (split_ratio[1] + split_ratio[2]), stratify=temp_labels\n",
    "    )\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs(os.path.join(save_path, 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_path, 'valid'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_path, 'test'), exist_ok=True)\n",
    "\n",
    "    # Save splits\n",
    "    np.save(os.path.join(save_path, 'train', 'data.npy'), train_data)\n",
    "    np.save(os.path.join(save_path, 'train', 'labels.npy'), train_labels)\n",
    "    np.save(os.path.join(save_path, 'valid', 'data.npy'), valid_data)\n",
    "    np.save(os.path.join(save_path, 'valid', 'labels.npy'), valid_labels)\n",
    "    np.save(os.path.join(save_path, 'test', 'data.npy'), test_data)\n",
    "    np.save(os.path.join(save_path, 'test', 'labels.npy'), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1093e315-7178-4b4d-86a3-7d2f0baa8968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(normal_folder, vf_folder, save_path):\n",
    "    normal_data = []\n",
    "    normal_labels = []\n",
    "    vf_data = []\n",
    "    vf_labels = []\n",
    "\n",
    "    # Process normal data\n",
    "    for file_name in os.listdir(normal_folder):\n",
    "        if file_name.endswith('.npy'):\n",
    "            file_path = os.path.join(normal_folder, file_name)\n",
    "            normal_data.append(np.load(file_path))\n",
    "            normal_labels.append(0)  # Label for normal data\n",
    "\n",
    "    # Process VF data\n",
    "    for file_name in os.listdir(vf_folder):\n",
    "        if file_name.endswith('.npy'):\n",
    "            label = int(file_name.split('_')[0])  # Extract label from file name\n",
    "            file_path = os.path.join(vf_folder, file_name)\n",
    "            vf_data.append(np.load(file_path))\n",
    "            vf_labels.append(1)  # Label for VF data\n",
    "\n",
    "    normal_data = np.array(normal_data)\n",
    "    normal_labels = np.array(normal_labels)\n",
    "    vf_data = np.array(vf_data)\n",
    "    vf_labels = np.array(vf_labels)\n",
    "\n",
    "    # Combine data\n",
    "    combined_data = np.concatenate([normal_data, vf_data], axis=0)\n",
    "    combined_labels = np.concatenate([normal_labels, vf_labels], axis=0)\n",
    "\n",
    "    # Split and save\n",
    "    split_and_save_data(combined_data, combined_labels, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad4b9752-7207-4c43-a25f-3cd5fbe3ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(folder_path, batch_size, shuffle_buffer_size=1000, repeat=False):\n",
    "    data = np.load(os.path.join(folder_path, 'data.npy'))\n",
    "    labels = np.load(os.path.join(folder_path, 'labels.npy'))\n",
    "    \n",
    "    # NaN 값이 있는 인덱스 찾기\n",
    "    nan_indices = np.any(np.isnan(data), axis=1)  # axis=1은 시퀀스 차원 기준\n",
    "\n",
    "    # NaN 값을 포함하는 인덱스\n",
    "    nan_indices = np.where(nan_indices)[0]\n",
    "\n",
    "    # NaN 값을 포함하는 인덱스를 제외하기\n",
    "    mask = np.ones(data.shape[0], dtype=bool)\n",
    "    mask[nan_indices] = False\n",
    "    \n",
    "    data_cleaned = data[mask]\n",
    "    labels_cleaned = labels[mask]\n",
    "    \n",
    "    # 레이블을 (batch_size, 1) 형태로 변환\n",
    "    labels_cleaned = np.expand_dims(labels_cleaned, axis=-1)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data_cleaned, labels_cleaned))\n",
    "    \n",
    "    if shuffle_buffer_size > 0:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5b22ebe0-8303-4463-b88e-10bc7c14a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측 시간 입력\n",
    "\n",
    "pred_min = 60\n",
    "if pred_min == 30:\n",
    "    vf_folder = '/smc_work/datanvme/VF/vf_before_min_30_60/'\n",
    "elif pred_min > 30:\n",
    "    vf_folder = '/smc_work/datanvme/VF/vf_before_min_60_120/'\n",
    "elif pred_min > 60:\n",
    "    vf_folder = '/smc_work/datanvme/VF/vf_before_min_120_180/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f262e95d-9ff4-40e7-adbd-494869614455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/smc_work/datanvme/VF/vf_before_min_60_120/'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vf_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e561bc1d-ff25-4039-8ca6-4fa45e8415e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "normal_folder = '/smc_work/datanvme/VF/normal/'\n",
    "save_path = '/smc_work/datanvme/VF/split_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "334703c2-b173-43db-8338-bd6485355ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_folder = '/smc_work/datanvme/VF/vf_before_min_120_180/' # 60 min ~ 120 min\n",
    "vf_folder = '/smc_work/datanvme/VF/vf_before_min_30_60/' # 30 min ~ 60 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdefc1c1-934f-4526-b0eb-de26ed906f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute data processing\n",
    "load_and_process_data(normal_folder, vf_folder, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28cd27b5-901f-4ab8-8e2a-c3412865e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(folder_path):\n",
    "    labels = np.load(os.path.join(folder_path, 'labels.npy'))\n",
    "    print(\"Loaded labels shape:\", labels.shape)\n",
    "    print(\"Sample labels:\", labels[:10])  # 처음 10개 레이블 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b44b09c-58de-477d-af45-717df7bbb80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 하이퍼파라미터\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "num_heads = 4\n",
    "d_ff = 512\n",
    "max_len = 2560  # 시퀀스 길이\n",
    "num_classes = 1  \n",
    "dropout_rate = 0.1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e4aa0aa-2f70-4200-9a3f-d2792b78f635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(os.path.join(save_path, 'train'), batch_size)\n",
    "valid_dataset = load_dataset(os.path.join(save_path, 'valid'), batch_size, repeat=False)\n",
    "test_dataset = load_dataset(os.path.join(save_path, 'test'), batch_size, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd3844c1-8deb-4d75-840d-9fdfc4e78944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 및 컴파일\n",
    "optimizer = Adam(learning_rate=1e-4)  # 기존보다 더 낮은 학습률로 설정\n",
    "model = TransformerModel(num_layers, d_model, num_heads, d_ff, max_len, num_classes, dropout_rate)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4f8d8a6-d020-499f-8231-4c82b43a1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 경로 생성\n",
    "model_save_path = f'/smc_work/home/weladmin/Desktop/code/Research/shea/VF/models/transformer_dmodel{d_model}_heads{num_heads}_layers{num_layers}_dropout{dropout_rate:.1f}'\n",
    "\n",
    "# 폴더 생성\n",
    "os.makedirs(model_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d44c7678-ac46-419d-9144-0949035fe7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/smc_work/home/weladmin/Desktop/code/Research/shea/VF/models/transformer_dmodel128_heads4_layers4_dropout0.1'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80937ce0-df1d-4602-8bc8-5571d124965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelCheckpoint 콜백 정의\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(model_save_path, 'model_epoch_{epoch:02d}_val_loss_{val_loss:.4f}'),  # 모델 저장 경로\n",
    "    monitor='val_loss',  # 모니터링할 지표\n",
    "    save_best_only=True,  # 가장 좋은 모델만 저장\n",
    "    mode='min',  # 최소값을 기준으로 저장\n",
    "    verbose=1  # 로그 출력\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60c29992-954a-484c-bc43-2c940888951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 조기 종료 콜백 정의\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # 모니터링할 지표\n",
    "    patience=3,  # patience는 몇 epoch동안 개선이 없을 때 종료할 지점\n",
    "    mode='min',  # 최소값을 기준으로 조기 종료\n",
    "    verbose=1  # 로그 출력\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4870409-83d5-45ee-8ee2-2f435877d3c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.7259 - accuracy: 0.5701\n",
      "Epoch 00001: val_loss improved from inf to 0.67209, saving model to /smc_work/home/weladmin/Desktop/code/Research/shea/VF/models/transformer_dmodel128_heads4_layers4_dropout0.1/model_epoch_01_val_loss_0.6721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_4_layer_call_fn, multi_head_attention_4_layer_call_and_return_conditional_losses, positionwise_feedforward_4_layer_call_fn, positionwise_feedforward_4_layer_call_and_return_conditional_losses, layer_normalization_8_layer_call_fn while saving (showing 5 of 220). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /smc_work/home/weladmin/Desktop/code/Research/shea/VF/models/transformer_dmodel128_heads4_layers4_dropout0.1/model_epoch_01_val_loss_0.6721/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /smc_work/home/weladmin/Desktop/code/Research/shea/VF/models/transformer_dmodel128_heads4_layers4_dropout0.1/model_epoch_01_val_loss_0.6721/assets\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7fd760d04e80> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7fd6fac52dc0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7fd6fa6ba190> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7fd6f98d16d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 102s 892ms/step - loss: 0.7259 - accuracy: 0.5701 - val_loss: 0.6721 - val_accuracy: 0.6181\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.5938\n",
      "Epoch 00002: val_loss improved from 0.67209 to 0.66840, saving model to /smc_work/home/weladmin/Desktop/code/Research/shea/VF/models/transformer_dmodel128_heads4_layers4_dropout0.1/model_epoch_02_val_loss_0.6684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_4_layer_call_fn, multi_head_attention_4_layer_call_and_return_conditional_losses, positionwise_feedforward_4_layer_call_fn, positionwise_feedforward_4_layer_call_and_return_conditional_losses, layer_normalization_8_layer_call_fn while saving (showing 5 of 220). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /smc_work/home/weladmin/Desktop/code/Research/shea/VF/models/transformer_dmodel128_heads4_layers4_dropout0.1/model_epoch_02_val_loss_0.6684/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /smc_work/home/weladmin/Desktop/code/Research/shea/VF/models/transformer_dmodel128_heads4_layers4_dropout0.1/model_epoch_02_val_loss_0.6684/assets\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7fd760d04e80> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7fd6fac52dc0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7fd6fa6ba190> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7fd6f98d16d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 98s 889ms/step - loss: 0.6841 - accuracy: 0.5938 - val_loss: 0.6684 - val_accuracy: 0.6186\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6754 - accuracy: 0.6088\n",
      "Epoch 00003: val_loss did not improve from 0.66840\n",
      "110/110 [==============================] - 90s 820ms/step - loss: 0.6754 - accuracy: 0.6088 - val_loss: 0.6687 - val_accuracy: 0.6186\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6725 - accuracy: 0.6095\n",
      "Epoch 00004: val_loss did not improve from 0.66840\n",
      "110/110 [==============================] - 90s 822ms/step - loss: 0.6725 - accuracy: 0.6095 - val_loss: 0.6908 - val_accuracy: 0.6186\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.6730 - accuracy: 0.6160\n",
      "Epoch 00005: val_loss did not improve from 0.66840\n",
      "110/110 [==============================] - 90s 822ms/step - loss: 0.6730 - accuracy: 0.6160 - val_loss: 0.6703 - val_accuracy: 0.6186\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea4dbc1-89ba-4213-b494-3d1bbcc263d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# 이미지를 저장할 함수 정의\n",
    "def save_image_with_dpi(image_array, file_path, dpi=(1200, 1200)):\n",
    "    # NumPy 배열을 이미지로 변환\n",
    "    image = Image.fromarray(image_array.astype('uint8'))\n",
    "\n",
    "    # 이미지 저장 시 DPI 설정\n",
    "    image.save(file_path, dpi=dpi)\n",
    "\n",
    "# 예시: ROC 곡선을 그린 후 이미지 저장\n",
    "def plot_and_save_roc_curve(fpr, tpr, auc, file_path):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # 현재 플롯을 이미지로 저장\n",
    "    plt.savefig(file_path, dpi=(1200, 1200))\n",
    "    plt.close()\n",
    "\n",
    "# 예시 데이터 (실제 사용 시 fpr, tpr, auc 값을 넣으세요)\n",
    "fpr = np.linspace(0, 1, 100)\n",
    "tpr = np.linspace(0, 1, 100)\n",
    "auc = 0.95\n",
    "\n",
    "# ROC 곡선을 그린 후 1200 DPI로 이미지 저장\n",
    "plot_and_save_roc_curve(fpr, tpr, auc, str(pred_min) + '_min_roc_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1cb4fb05-379a-42dd-b974-97fecda1e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에서 예측\n",
    "def predict_and_evaluate(model, test_dataset):\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.evaluate(test_dataset)  # 정확도 출력\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for x_batch, y_batch in test_dataset:\n",
    "        y_true.extend(y_batch.numpy().flatten())\n",
    "        y_pred.extend(model.predict(x_batch).flatten())\n",
    "        \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    # AUC 계산\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    print(f\"Test AUC: {auc:.4f}\")\n",
    "\n",
    "    # ROC 곡선 계산\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    \n",
    "    # ROC 곡선 시각화\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7c44f-3f55-442e-a9f8-41b3b084c6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 74s 294ms/step - loss: 0.0173 - accuracy: 0.9946\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가 및 AUC 시각화\n",
    "predict_and_evaluate(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff3da2f-1621-4d77-a8b3-de6cfa298aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 중간 출력을 확인하는 코드\n",
    "class DebugModel(tf.keras.Model):\n",
    "    def __init__(self, original_model):\n",
    "        super(DebugModel, self).__init__()\n",
    "        self.model = original_model\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        x = self.model.embedding(x)\n",
    "        print(f\"Embedding output shape: {x.shape}\")  # 디버깅\n",
    "        x = self.model.positional_encoding(x)\n",
    "        print(f\"After positional encoding shape: {x.shape}\")  # 디버깅\n",
    "        for block in self.model.transformer_blocks:\n",
    "            x = block(x, mask, training=training)\n",
    "            print(f\"After transformer block shape: {x.shape}\")  # 디버깅\n",
    "        x = self.model.dropout(x, training=training)\n",
    "        print(f\"After dropout shape: {x.shape}\")  # 디버깅\n",
    "        x = self.model.final_layer(x)\n",
    "        print(f\"Final output shape: {x.shape}\")  # 디버깅\n",
    "        return x\n",
    "\n",
    "debug_model = DebugModel(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
